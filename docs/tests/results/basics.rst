
.. _tests.results:

Test Results
============

Every successful test run generates a set of results in JSON. These are
saved with the test, but are also logged to a central ``results.log``
file that is formatted in a Splunk compatible manner.

This page offers an overview of Pavilion's result handling features.

.. contents:

Result Logs
-----------

There are three different result logs:

per-test-run results json
~~~~~~~~~~~~~~~~~~~~~~~~~

The result json is written to a file named ``results`` in the test run
directory. This is used to recall results from test runs.

per-test-run result log
~~~~~~~~~~~~~~~~~~~~~~~

This results log records every step of the results gathering process in great
detail. It is stored in each test run directory in ``results.log``, and is
available via the ``pav log results <test_id>`` command.

Pavilion-wide result log
~~~~~~~~~~~~~~~~~~~~~~~~

The general results log keeps a record of the results of every test run under
your instance of Pavilion. It's location is configured via the general
``pavilion.yaml`` config, but defaults to residing in the working directory.
It's format is designed to be easily read by Splunk and similar tools.

Gathering Results
-----------------

Result gathering steps:

1) Generate the base result field values.
2) Use result parser plugins to parse values from result files.
3) Use result evaluations to modify results or generate additional values.
4) Convert the 'result' key from True/False to 'PASS'/'FAIL'.

All results are stored in a json mapping that looks like this:

.. code-block:: json

    {"avg_speed": 3.7999,
     "command_test": "PASS",
     "created": "2020-07-06 15:28:08.528899",
     "duration": "0:00:06.130584",
     "finished": "2020-07-06 15:28:08.528617",
     "id": 16176,
     "job_id": "123",
     "name": "mytests.base",
     "pav_result_errors": [],
     "result": "PASS",
     "return_value": 0,
     "sched": {"avail_mem": "80445",
               "cpus": "36",
               "free_mem": "76261",
               "min_cpus": "36",
               "min_mem": "131933.144",
               "total_mem": "125821"},
     "started": "2020-07-06 15:28:02.398033",
     "sys_name": "pav-test",
     "per_file": {
        "node01": {"raw_speed": 33},
        "node03": {"raw_speed": 39},
        "node05": {"raw_speed": 42}
     }
     "user": "bob"}

- Most values are stored at the top level of the mapping.
- Most scheduler variables are included, to improve tracking of how exactly
  the test ran.
- ``pav_result_errors`` stores a list of errors encountered when gathering
  results.
- The 'per_file' section stores information gathered across multiple
  results files This can be used to
  gather separate per-node information in cases where you have an output file
  for each node when using result parsers.
- Average speed in this case is a value calculated from each of the node speeds
  using *result evaluations*.
- The ``flatten_results`` key in the pavilion.log file can be used to convert
  results with multiple ``per_file`` results into a corresponding number of
  separate log entries.


Basic Result Keys
~~~~~~~~~~~~~~~~~

These keys are present in the results for every test, whether the test
passed or failed. To see the latest list of base result values, run
``pav show result_base``. All of these keys, as well as 'result', are reserved.

.. code-block:: text

    $ pav show result_base
     Name              | Doc
    -------------------+-------------------------------------------------
     name              | The test run name
     id                | The test run id
     created           | When the test was created.
     started           | When the test run itself started.
     finished          | When the test run finished.
     duration          | Duration of the test run (finished - started)
     user              | The user that started the test.
     job_id            | The scheduler plugin's jobid for the test.
     sched             | Most of the scheduler variables.
     sys_name          | The system name '{{sys.sys_name}}'
     pav_result_errors | Errors from processing results.
     per_file          | Per filename results.
     return_value      | The return value of run.sh

All time fields are in ISO8601 format.

Additionally, the 'file' key is reserved.

Basic Result Parsing
~~~~~~~~~~~~~~~~~~~~

*This is a quick summary for context.
 For full result parser documentation, see ref:`tests.results.result_parsers`*.

The ``result_parse`` section of the config lets you configure result parsers
to pull useful information out of files generated by your test run.

Result parsers are plugins. Pavilion comes with several that are useful in
common scenarios, but tests with more complex results may warrant the
development of a custom result parser to specially handle that test's output.
To see the currently available result parsers, use ``pav show parsers``.

.. code-block:: bash

    $ pav show result_parsers
     Available Result Parsers
    -----------+---------------------------------------------------------------
     Name      | Description
    -----------+---------------------------------------------------------------
     command   | Runs a given command.
     filecheck | Checks the working directory for a given file. The parser
               | will tell the user if the filename exists or not.
     constant  | Set a constant as result.
     table     | Parses tables.
     split     | Split a line by some substring, and return the list of parts.
     regex     | Find matches to the given regex in the given file. The
               | matched string or strings are returned as the result.

Each of these is configured using its own configuration section under
``result parse``. To see the full configuration documentation for a parser,
use ``pav show result_parsers --config <parser_name>``. For easier to read
documentation, use ``pav show result_parsers --doc <parser_name>``.

.. code:: yaml

    mytest:
      scheduler: raw
      run:
        cmds:
          - ping -c 10 google.com

      result_parse:
          # The results.parse section is comprised of configs for result parsers,
          # identified by name. Each parser type section is further comprised
          # of a mapping of key to
          regex:
            # Each result parser can have multiple configs.

            # The value matched will be stored in this key
            loss:
              # This tells the regex parser what regular expression to use.
              # Single quotes are recommended, as they are literal in yaml.
              # By default this will try to match every line in the
              # output of the test run script (and by extension everything
              # printed by your ``run.cmds`` section.
              regex: '\d+% packet loss'

- Each result parser type can define multiple keys.
- The found value is stored at the top level of the result JSON (by default).
- All result parser keys must be unique.

.. _tests.results.basics.evaluations:

Basic Result Evaluations
~~~~~~~~~~~~~~~~~~~~~~~~

*Advanced Features* :ref:`tests.results.evaluate`

The ``result_evaluate`` section of a test config allows for manipulating
results produced from result parsers and other keys in the ``result_evaluate``
section.

 - Keys must simple (no multi-valued keys like in result_parsers)
 - The syntax is almost identical to :ref:`tests.values.expressions`,
   - They are resolved with the same sub-system, with minor differences.
 - Variables in the expressions are result key values.

.. code-block:: yaml

    eval_example:
        run:
            cmds:
                - time wget google.com

        # This will produce results that include the line:
        # real   0m3.256s

        result_parse:
            regex:
                # Parse out the minutes and seconds
                real_m:
                    regex: 'real\s+(\d+)m'
                real_s:
                    regex: 'real\s\d+m([0-9.]+)s'

        result_evaluate:
            # combine the minutes and seconds that we parsed out
            # into one number of seconds.
            real: 'real_m * 60 + real_s'
            # These can reference values set by other evaluations.
            real_hours: 'real/60/60'

Errors
~~~~~~

If an error occurs when parsing results that can be recovered from, a
description of the error is recorded under the ``error`` key. Each of
these is a dictionary with some useful values:

.. code:: yaml

    {
      "errors": [{
        # The error happened under this parser.
        "result_parser": "regex",
        # The file being processed.
        "file": "node3.out",
        # The key being processed
        "key": "hugetlb",
        "msg": "Error reading file 'node3.out': Permission error"
      }]
    }

The Test Result
~~~~~~~~~~~~~~~

The 'result' key denotes the final test result, and will always be
either '**PASS**', '**FAIL**' or '**ERROR**'.  **ERROR** in this case means
the test had a non-recoverable error when checking whether the test
passed or failed.

You can set the 'result' using either result parsers or result evaluations.
It must be set as a single True or False value.

- For result parsers, this means you should use an 'action' of 'store_true'
  (the default) or 'store_false' (See :ref:`tests.results.actions`). You will
  also need to use a 'per_file' setting that produces a single value, like
  'first' or 'all' (See :ref:`tests.results.per_file`).
- For result evaluations this simply means ensuring that the evaluation
  returns a boolean, typically by way of a comparison operator.

If you don't set the 'result' key yourself, Pavilion defaults to adding the
evaluation: ``result: 'return_value == 0'``. This is why, by default,
Pavilion test runs **PASS** if the run script returns 0.
